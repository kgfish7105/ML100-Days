--------------------------HW_1-----------------------
--------------------------HW_03-----------------------
讀檔 (非CSV的資料)

--------------------------HW_04-----------------------
DataFrame中最常?的欄位資料類型有三種:
float64 ： 浮點數，可表示離散或連續變數
int64 ： 整數，可表示離散或連續變數
object ： 包含字串，用於表示類別型變數

--------------------------HW_05-----------------------
計算資料分散程度: Min、Max、Range、Quartiles、Var、Std
計算集中趨勢
--------------------------HW_06-----------------------
Outlier 及處理 (視情況以中位數, Min, Max 或平均數填補(有時會用 NA))
繪製散點圖 (scatter)、分布圖 (histogram) 或其他圖(boxplot)檢查是否有異常

--------------------------HW_07-----------------------
常用的數值取代：中位數與分位數 連續數值標準化
np.quantile、np.median

--------------------------HW_08-----------------------
分群 groupby
cut, value_counts
DataFrame應用
df.loc[cond, 'col_name']
sub_df = df[['col1','col2']]
plt.boxplot(column=plt_column, by = plt_by)
Z轉換: 亦即將觀察值與母體平均數之間的距離，以標準差為單位來計算 (標準化)
--------------------------HW_09-----------------------
plt.scatter 看兩變數的相關性
正相關
負相關
--------------------------HW_10-----------------------
plt.boxplot 看有沒有outlier
seaborn.heatmap(相關係數)
seaborn.pairplot(相關係數)
--------------------------HW_11-----------------------
了解變數分布狀態: Bar & KDE (density plot)
np.linspace
sort_values(by='col_name')['col_name']
seaborn.barplot
seaborn.distplot (分布圖): 集合了matplotlib的 hist()、核密度函數kdeplot的功能
--------------------------HW_12-----------------------
連續資料離散化
cut, qcut, value_counts

--------------------------HW_13-----------------------
seaborn.catplot

--------------------------HW_14-----------------------
subplot
warnings.filterwarnings
np.sort
cut, groupby

--------------------------HW_15-----------------------
rand, randint, uniform, randn, normal

seaborn.heatmap
seaborn.PairGrid
kdeplot
--------------------------HW_16-----------------------
Imputer
MinMaxScaler
Fit the model
LogisticRegression
--------------------------HW_17-----------------------
LinearRegression.fit(train_X, train_Y)
train_Y = 轉換後的label

--------------------------HW_18-----------------------
常見資料型態: float、int、object
型態分類: type、int(var)、series.astype(int)

--------------------------HW_19-----------------------
<房價預測>
cross_val_score: 交叉驗證

填補缺失值
填補平均值(Mean) : 數值型欄位，偏態不明顯
• 填補中位數(Median) : 數值型欄位，偏態很明顯
• 填補眾數(Mode) : 類別型欄位

補不可能出現的數值 : 類別型欄位，但不適合用眾數時(篇態不明顯)

標準化 / 最小最大化 使用上的差異
標準化 : 轉換不易受到極端值影響
最小最大化 : 轉換容易受到極端值影響
所以 去除離群值的特徵，比較適和 最小最大化

非樹狀模型:  如線性迴歸, 羅吉斯迴歸, 類神經...等，標準化 / 最小最大化後對預測會有影響
樹狀模型 : 如決策樹, 隨機森林, 梯度提升樹...等，標準化 / 最小最大化後對預測不會有影響

邏輯回歸（Logistic Regression）是延伸自線性回歸（Linear Regression）的一種變形。 (「線性迴歸 + Sigmoid 函數」)
「回歸」一般來說指的是輸出變量為連續值的方法，而「分類」的輸出變量是離散型（Discrete）的。
所以實際上，邏輯回歸是用於分類的方法，而不是回歸。

--------------------------HW_20-----------------------
處理離峰值方法:
1. 調整離峰值 df['col'].clip(0,2500)
2. 捨棄離峰值 (df['col']> 0) & (df['col']< 2500)

--------------------------HW_21-----------------------
去除偏態:
薪資分布中，高薪群的長尾分布造成平均值不具代表性
但是對數去偏後的新分布，平均值就比較具有代表性
方根去偏(sqrt)
分布去偏(coxbox) 
對數去偏(log1p)   

--------------------------HW_22-----------------------
基礎編碼 1 : 標籤編碼 ( Label Encoding ): 耗時小。準確普通(適用非樹狀模型)      
基礎編碼 2 : 獨熱編碼 ( One Hot Encoding ): 耗時大，準確高一點(適用非樹狀模型)

--------------------------HW_23-----------------------
均值編碼: 使目標值的平均值，取代原本的類別型特徵

df 縱向合併用 concat
df 橫向合併用 merge

提醒 :均值編碼容易 overfitting
當平均值的可靠度低時, 我們會傾向相信 全部的總平均
當平均值的可靠度高時, 我們會傾向相信 類別的平均

均值編碼平滑化

--------------------------HW_24-----------------------                   
計數編碼 ( Counting )   
特徵雜湊 ( Feature Hash ) 

如果類別的目標均價與類別筆數呈正相關 ( 或負相關 )，也可以將筆數本身當成特徵
例如 : 購物網站的消費金額預測

觀察欄位相異值數量   pd.Series.nunique
pd.merge                      
  
  
--------------------------HW_25-----------------------  
時間特徵提取 
最直覺的?式，就是依照原意義分欄處理，或加上第幾周或星期幾
但某些欄(例 : 分、秒)與目標值的關係很低
考慮目標值是否與 哪幾種時間週期 有關
例如: 月週期 與 開銷 有關

--------------------------HW_26-----------------------
<增加特徵>
特徵組合 - 數值與數值組合
最典型的例子就是經、緯度兩個欄位分別代表方向的距離，合在一個看可以帶出一個位置

機器學習的關鍵在特徵工程
特徵工程的關鍵在領域知識

--------------------------HW_27-----------------------
<增加特徵>
群聚編碼(特徵組合) - 數值 與 離散組合 (沒有動用目標值)
有一個有趣的例子是說，如果有一個小嬰兒的資料集，
其中包含「情緒」跟「時間」的欄位，分開看沒什麼意義。
合在一起看就會變成「白天哭」跟「晚上哭」這樣的欄位，可能更容易看出什麼

--------------------------HW_28----------------------- 
<減少特徵>
<特徵選擇>
GDBＴ(梯度提升樹) 嵌入法

--------------------------HW_29-----------------------
特徵重要性評估(Feature Importance) 計算原理必須基於樹狀模型(用來塞選出哪些特徵影響程度大)     
排序重要性評估(Permutation Importance) 可延伸 非樹狀模型
    
預設方式是取 特徵決定分支的次數 
還有兩種更直覺的特徵重要性 : 特徵覆蓋度、損失函數降低量 (loss function)

--------------------------HW_30-----------------------
<分類預測的集成>
葉編碼 (leaf encoding)
                  


--------------------------HW_34-----------------------
K-fold Cross-validation (random_state: 確保每次得到結果固定)
SK learn的 train-test split函數 (test_size 可以是0-, float 也可以是int)

np.append(array1, array2)

驗證集常用來評估不同超參數或不同模型的結果


--------------------------HW_35-----------------------
多標籤分類問題 (Multi-label)

--------------------------HW_36-----------------------
評估指標 - 分類
AUC, Area Under Curve, 範圍: [0, 1] ((通常為機率 > 0.5 判定為 1, 機率 < 0.5 判定為 0))
F1 - Score (Precision, Recall), 範圍: [0, 1] (分類問題中，我們有時會對某一類別的準確率特別感興趣)
	- Precision，Recall 則是針對某類別進行評估
	- Precision: 模型判定瑕疵，樣本確實為瑕疵的比例
	- Recall: 模型判定的瑕疵，佔樣本所有瑕疵的比例
	F1-Score 則是 Precision, Recall 的調和平均數


--------------------------HW_37-----------------------
Regression 模型

--------------------------HW_38-----------------------
Linear Regression
Logisitic Regression 模型應用


--------------------------HW_39-----------------------
機器學習-目標函數元素:
--損失函數 Loss function
--正規化 Regularization (避免模型過於複雜->Overfitting)

目標函數 = Loss function + Regularization(懲罰項)

Lasso 可以當作嵌入式的特徵選擇
該方法給回歸係數加入了L1懲罰，導致其中的許多參數趨於零。任何回歸係數不為零的特徵都會被LASSO算法「選中」
嵌入法最大的突破在於，特徵選擇會在學習器的訓練過程中自動完成

--------------------------HW_40-----------------------
LASSO 與 Ridge :用於修正Linear model的實作

"scatter畫圖不同顏色" royalblue、yellowgreen、indianred

可以看見 LASSO 與 Ridge 的結果並沒有比原本的線性回歸來得好， 
這是因為目標函數被加上了正規化函數，讓模型不能過於複雜，相當於限制模型擬和資料的能力。
因此若沒有發現 Over-fitting 的情況，是可以不需要一開始就加上太強的正規化的。

